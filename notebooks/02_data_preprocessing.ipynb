{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIC IoMT 2024 Dataset - Data Preprocessing\n",
        "\n",
        "This notebook handles data preprocessing steps:\n",
        "- **Duplicate Removal**: Clean duplicate rows from datasets\n",
        "- **Label Encoding**: Convert categorical labels to numerical format\n",
        "- **Feature Scaling**: Normalize/standardize features for model training\n",
        "- **Train/Validation Split**: Split training data into train and validation sets\n",
        "- **Data Saving**: Save preprocessed datasets for model training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import joblib\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add src to path for imports\n",
        "BASE_DIR = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "sys.path.insert(0, str(BASE_DIR / 'src'))\n",
        "\n",
        "# Import common utilities\n",
        "from utils import get_project_paths, load_datasets, remove_exact_duplicates, comprehensive_outlier_check\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 10)\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset and Setup Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get project paths and load datasets\n",
        "paths = get_project_paths()\n",
        "DATA_DIR = paths['DATA_DIR']\n",
        "OUTPUT_DIR = paths['OUTPUT_DIR']\n",
        "MODELS_DIR = paths['MODELS_DIR']\n",
        "\n",
        "# Load datasets\n",
        "train_df, test_df = load_datasets()\n",
        "print(f\"✓ Training: {train_df.shape} | Test: {test_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Duplicate Removal\n",
        "\n",
        "**Note**: From EDA, we found 2.6M+ duplicate rows. We'll apply the duplicate removal function here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function is imported from utils.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply duplicate removal\n",
        "print(\"Removing duplicates...\")\n",
        "train_df, train_stats = remove_exact_duplicates(train_df, keep='first', return_stats=True)\n",
        "test_df, test_stats = remove_exact_duplicates(test_df, keep='first', return_stats=True)\n",
        "print(f\"✓ Training: {len(train_df):,} rows | Test: {len(test_df):,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Separate Features and Labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and labels\n",
        "feature_cols = [col for col in train_df.columns if col != 'label']\n",
        "\n",
        "X_train = train_df[feature_cols]\n",
        "y_train = train_df['label']\n",
        "\n",
        "X_test = test_df[feature_cols]\n",
        "y_test = test_df['label']\n",
        "\n",
        "print(f\"✓ {len(feature_cols)} features | Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Label Encoding\n",
        "\n",
        "Convert categorical labels to numerical format for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "class_names = label_encoder.classes_\n",
        "n_classes = len(class_names)\n",
        "\n",
        "print(f\"✓ {n_classes} classes encoded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.1. Outlier Detection\n",
        "\n",
        "Detect outliers to determine the best scaling method.\n",
        "\n",
        "**Note**: The `comprehensive_outlier_check()` function is imported from `src/utils.py` (see imports at the top of the notebook).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function is imported from utils.py\n",
        "\n",
        "# Check outliers for all features\n",
        "print(\"Detecting outliers...\")\n",
        "outlier_results = comprehensive_outlier_check(X_train, feature_cols, methods=['iqr', 'zscore'])\n",
        "\n",
        "# Calculate summary statistics\n",
        "iqr_outlier_percentages = [outlier_results[col]['iqr']['percentage'] for col in feature_cols]\n",
        "zscore_outlier_percentages = [outlier_results[col]['zscore']['percentage'] for col in feature_cols]\n",
        "\n",
        "avg_iqr_outliers = np.mean(iqr_outlier_percentages)\n",
        "avg_zscore_outliers = np.mean(zscore_outlier_percentages)\n",
        "\n",
        "print(f\"✓ Outlier detection completed\")\n",
        "print(f\"  Average IQR outliers: {avg_iqr_outliers:.2f}%\")\n",
        "print(f\"  Average Z-score outliers: {avg_zscore_outliers:.2f}%\")\n",
        "\n",
        "# Show top 10 features with most outliers\n",
        "outlier_summary = []\n",
        "for col in feature_cols:\n",
        "    iqr_pct = outlier_results[col]['iqr']['percentage']\n",
        "    zscore_pct = outlier_results[col]['zscore']['percentage']\n",
        "    outlier_summary.append((col, iqr_pct, zscore_pct))\n",
        "\n",
        "outlier_summary.sort(key=lambda x: x[1], reverse=True)\n",
        "print(f\"\\nTop 10 features with most outliers (IQR method):\")\n",
        "for col, iqr_pct, zscore_pct in outlier_summary[:10]:\n",
        "    print(f\"  {col:<30s}: IQR={iqr_pct:6.2f}% | Z-score={zscore_pct:6.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "class_counts = pd.Series(y_train_encoded).value_counts().sort_index()\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.bar(range(len(class_counts)), class_counts.values, color='steelblue', alpha=0.7)\n",
        "plt.xlabel('Class Index', fontsize=12)\n",
        "plt.ylabel('Number of Samples', fontsize=12)\n",
        "plt.title('Class Distribution After Encoding', fontsize=14, fontweight='bold')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Class stats: Min={class_counts.min():,} | Max={class_counts.max():,} | Mean={class_counts.mean():.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Scaling (Auto-selected based on Outliers)\n",
        "\n",
        "Scaling method is automatically selected based on outlier detection:\n",
        "- **RobustScaler**: If average outliers > 5% (robust to outliers)\n",
        "- **StandardScaler**: If average outliers ≤ 5% (good for normal distributions)\n",
        "- **MinMaxScaler**: Alternative option (scales to [0,1] range)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auto-select scaling method based on outlier detection\n",
        "OUTLIER_THRESHOLD = 5.0  # Percentage threshold\n",
        "\n",
        "if avg_iqr_outliers > OUTLIER_THRESHOLD:\n",
        "    SCALING_METHOD = 'robust'\n",
        "    scaler = RobustScaler()\n",
        "    reason = f\"High outliers detected ({avg_iqr_outliers:.2f}%)\"\n",
        "elif avg_zscore_outliers > OUTLIER_THRESHOLD:\n",
        "    SCALING_METHOD = 'robust'\n",
        "    scaler = RobustScaler()\n",
        "    reason = f\"High outliers detected ({avg_zscore_outliers:.2f}%)\"\n",
        "else:\n",
        "    SCALING_METHOD = 'standard'\n",
        "    scaler = StandardScaler()\n",
        "    reason = f\"Low outliers ({avg_iqr_outliers:.2f}%)\"\n",
        "\n",
        "print(f\"Selected scaling method: {SCALING_METHOD.upper()}\")\n",
        "print(f\"Reason: {reason}\")\n",
        "\n",
        "# Apply scaling\n",
        "X_train_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_train), \n",
        "    columns=feature_cols, \n",
        "    index=X_train.index\n",
        ")\n",
        "X_test_scaled = pd.DataFrame(\n",
        "    scaler.transform(X_test), \n",
        "    columns=feature_cols, \n",
        "    index=X_test.index\n",
        ")\n",
        "\n",
        "print(f\"✓ Scaling completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train/Validation Split\n",
        "\n",
        "Split the training data into train and validation sets for model evaluation during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Validation split\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_scaled,\n",
        "    y_train_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_train_encoded\n",
        ")\n",
        "\n",
        "print(f\"✓ Split: Train={len(X_train_final):,} | Val={len(X_val):,} | Test={len(X_test_scaled):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Preprocessed Data\n",
        "\n",
        "Save the preprocessed datasets and preprocessing objects for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save preprocessed datasets\n",
        "# Numpy arrays (for TensorFlow/Keras)\n",
        "np.save(OUTPUT_DIR / 'X_train.npy', X_train_final.values)\n",
        "np.save(OUTPUT_DIR / 'X_val.npy', X_val.values)\n",
        "np.save(OUTPUT_DIR / 'X_test.npy', X_test_scaled.values)\n",
        "np.save(OUTPUT_DIR / 'y_train.npy', y_train_final)\n",
        "np.save(OUTPUT_DIR / 'y_val.npy', y_val)\n",
        "np.save(OUTPUT_DIR / 'y_test.npy', y_test_encoded)\n",
        "\n",
        "# Parquet files (for pandas/analysis)\n",
        "X_train_final.to_parquet(OUTPUT_DIR / 'X_train.parquet')\n",
        "X_val.to_parquet(OUTPUT_DIR / 'X_val.parquet')\n",
        "X_test_scaled.to_parquet(OUTPUT_DIR / 'X_test.parquet')\n",
        "pd.Series(y_train_final).to_frame('label').to_parquet(OUTPUT_DIR / 'y_train.parquet')\n",
        "pd.Series(y_val).to_frame('label').to_parquet(OUTPUT_DIR / 'y_val.parquet')\n",
        "pd.Series(y_test_encoded).to_frame('label').to_parquet(OUTPUT_DIR / 'y_test.parquet')\n",
        "\n",
        "print(\"✓ Data files saved\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save preprocessing objects\n",
        "joblib.dump(scaler, MODELS_DIR / 'scaler.pkl')\n",
        "joblib.dump(label_encoder, MODELS_DIR / 'label_encoder.pkl')\n",
        "\n",
        "with open(MODELS_DIR / 'class_mapping.pkl', 'wb') as f:\n",
        "    pickle.dump({i: name for i, name in enumerate(class_names)}, f)\n",
        "\n",
        "with open(MODELS_DIR / 'preprocessing_info.pkl', 'wb') as f:\n",
        "    pickle.dump({\n",
        "        'scaling_method': SCALING_METHOD,\n",
        "        'n_features': len(feature_cols),\n",
        "        'n_classes': n_classes,\n",
        "        'feature_names': feature_cols,\n",
        "        'class_names': class_names.tolist(),\n",
        "        'train_size': len(X_train_final),\n",
        "        'val_size': len(X_val),\n",
        "        'test_size': len(X_test_scaled),\n",
        "        'train_stats': train_stats,\n",
        "        'test_stats': test_stats\n",
        "    }, f)\n",
        "\n",
        "print(\"✓ Preprocessing objects saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Train: {len(X_train_final):,} | Val: {len(X_val):,} | Test: {len(X_test_scaled):,}\")\n",
        "print(f\"Features: {len(feature_cols)} | Classes: {n_classes} | Scaling: {SCALING_METHOD}\")\n",
        "print(f\"✓ All files saved to {OUTPUT_DIR} and {MODELS_DIR}\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
